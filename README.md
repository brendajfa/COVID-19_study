
# Deep Learning models

## RNN
<p align="center">
    <img src="DL_Models/RNN.png">
</p>
<p align="center">
    <img src="DL_Models/RNN2.png">
</p>

---
## LSTM
<p align="center">
    <img src="DL_Models/LSTM_Cell.svg.png" width="80%">
</p>

---
## CNN
<p align="center">
    <img src="DL_Models/CNN.png" width="80%">
</p>

---
## GAN
<p align="center">
    <img src="DL_Models/GAN.png" width="90%">
</p>

---
## VAE
<p align="center">
    <img src="DL_Models/VAE.png">
</p>
<p align="center">
    <img src="DL_Models/VAE2.png">
</p>


---
---
# Machine Learning

## Regression
<p align="center">
    <img src="ML_Models/Regression.png" width="80%">
</p>

### Linear Regression
<p align="center">
    <img src="ML_models/Linear_regression.jpg" width="60%">
</p>

### Logistic Regression

Logistic regression is a predictive analysis technique used when the dependent variable is binary, like presence/absent, yes/no. Consider the simplest case with two predictors, $X_1$ and $X_2$, and a binary variable $Y$. Let p denote the probability that $Y = 1 (p = P(Y = 1))$. It is assumed a linear relationship between the predictor variables and the log-odds (also called logit) of the event that $Y = 1$. In statistics, the logit function is the logarithm of the odds (a measure of the likelihood of a particular outcome) of the result $\frac{p}{1-p}$. This relationship can be written as:
<p align="adjust">
    <img src="ML_models/Logistic_regression.png" width="90%">
</p>
<p align="center">
    <img src="ML_models/Logistic_regression_image.png" width="70%">
</p>

---
## Time Series Prediction
<p align="center">
    <img src="ML_models/time_series.png" width="70%">
</p>

---
## Classification

<p align="center">
    <img src="ML_models/class1.png" width="70%">
</p>

### Support Vector Machine (SVM) 

<p align="center">
    <img src="ML_models/svm.png" width="70%">
</p>

### Instance-based learning

<p align="center">
    <img src="ML_models/ibl.png" width="70%">
</p>

### Decisison Tree

<p align="center">
    <img src="ML_models/dts.png" width="70%">
</p>

<p align="center">
    <img src="ML_models/dt_image.png" width="80%">
</p>

<div align="justify">

### Ensemble Learning
**Random Forest** 
Random Forests (RFs) is a bagging ensemble learning method that generates several decision trees during the training phase and returns as result the mean prediction of the individual trees. Extremely Randomized Trees, also referred as extra trees, is another ensemble method that changes the tree generation by introducing more variation, such as tree depth.
Random forest models are often more accurate than single-tree models because they’re less prone to overfitting and they’re more robust against new types of data.

**Boosting** 

In boosting weak learners are sequentially combined in an adaptive way, i. e. each model gives more importance to the misclassified examples by assigning lower weights to correctly classified examples and higher weights to examples difficult to classify. **AdaBoost** [33] is the most known boosting method.

**Gradient Boosting (GB)** is an ensemble method that builds weak learners by optimizing a suitable cost function.**XGBoost** is an efficient implementation of Gradient Boosting which obtains more accurate predictions.
Stacking ensemble is a variation of ensemble learning whose main characteristic is the combination of different types of weak learners.

</div>

### Difference between Decision Tree and Random Forest

<p align="center">
    <img src="ML_models/diff_DT_RG.png" width="80%">
</p>


# Evaluation metrics

<p align="center">
    <img src="metrics/metrics.png" width="100%">
</p>

<p align="center">
    <img src="metrics/recall_accuracy.png" width="70%">
</p>


